{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Textmining with NLTK\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "For this code tutorial, you will need to install [nltk](https://anaconda.org/anaconda/nltk) and [wordcloud](https://anaconda.org/conda-forge/wordcloud).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>created_at</th>\n",
       "      <th>id</th>\n",
       "      <th>author_id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2021-12-10T07:20:45.000Z</td>\n",
       "      <td>1469205428227784711</td>\n",
       "      <td>44196397</td>\n",
       "      <td>@Albi_SideArms maybe i will ‚Ä¶</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2021-12-10T07:19:05.000Z</td>\n",
       "      <td>1469205011687223298</td>\n",
       "      <td>44196397</td>\n",
       "      <td>@jack https://t.co/ueyR6NAwap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2021-12-10T06:44:19.000Z</td>\n",
       "      <td>1469196261953884160</td>\n",
       "      <td>44196397</td>\n",
       "      <td>@SawyerMerritt ü§£ü§£</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2021-12-10T04:42:00.000Z</td>\n",
       "      <td>1469165476911755264</td>\n",
       "      <td>44196397</td>\n",
       "      <td>@SawyerMerritt Tesla China has done amazing work</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2021-12-10T04:21:25.000Z</td>\n",
       "      <td>1469160298158383109</td>\n",
       "      <td>44196397</td>\n",
       "      <td>@MrBeast üôè</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>67</td>\n",
       "      <td>2021-12-03T23:35:30.000Z</td>\n",
       "      <td>1466914018615078912</td>\n",
       "      <td>44196397</td>\n",
       "      <td>@joroulette It is an honor to serve NASA and t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>68</td>\n",
       "      <td>2021-12-03T19:28:57.000Z</td>\n",
       "      <td>1466851970443010056</td>\n",
       "      <td>44196397</td>\n",
       "      <td>@NASASpaceflight 39A is hallowed spaceflight g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>69</td>\n",
       "      <td>2021-12-03T19:22:34.000Z</td>\n",
       "      <td>1466850364012044288</td>\n",
       "      <td>44196397</td>\n",
       "      <td>@EvaFoxU @SawyerMerritt Huge cranes are cool haha</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>70</td>\n",
       "      <td>2021-12-03T19:20:15.000Z</td>\n",
       "      <td>1466849780253003782</td>\n",
       "      <td>44196397</td>\n",
       "      <td>@PPathole @ErcXspace @SpaceX This will look so...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>71</td>\n",
       "      <td>2021-12-03T19:18:44.000Z</td>\n",
       "      <td>1466849402534907910</td>\n",
       "      <td>44196397</td>\n",
       "      <td>@EvaFoxU @SawyerMerritt I love Norway! üá≥üá¥ ‚ô•Ô∏èüá≥üá¥</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>72 rows √ó 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0                created_at                   id  author_id  \\\n",
       "0            0  2021-12-10T07:20:45.000Z  1469205428227784711   44196397   \n",
       "1            1  2021-12-10T07:19:05.000Z  1469205011687223298   44196397   \n",
       "2            2  2021-12-10T06:44:19.000Z  1469196261953884160   44196397   \n",
       "3            3  2021-12-10T04:42:00.000Z  1469165476911755264   44196397   \n",
       "4            4  2021-12-10T04:21:25.000Z  1469160298158383109   44196397   \n",
       "..         ...                       ...                  ...        ...   \n",
       "67          67  2021-12-03T23:35:30.000Z  1466914018615078912   44196397   \n",
       "68          68  2021-12-03T19:28:57.000Z  1466851970443010056   44196397   \n",
       "69          69  2021-12-03T19:22:34.000Z  1466850364012044288   44196397   \n",
       "70          70  2021-12-03T19:20:15.000Z  1466849780253003782   44196397   \n",
       "71          71  2021-12-03T19:18:44.000Z  1466849402534907910   44196397   \n",
       "\n",
       "                                                 text  \n",
       "0                       @Albi_SideArms maybe i will ‚Ä¶  \n",
       "1                       @jack https://t.co/ueyR6NAwap  \n",
       "2                                   @SawyerMerritt ü§£ü§£  \n",
       "3    @SawyerMerritt Tesla China has done amazing work  \n",
       "4                                          @MrBeast üôè  \n",
       "..                                                ...  \n",
       "67  @joroulette It is an honor to serve NASA and t...  \n",
       "68  @NASASpaceflight 39A is hallowed spaceflight g...  \n",
       "69  @EvaFoxU @SawyerMerritt Huge cranes are cool haha  \n",
       "70  @PPathole @ErcXspace @SpaceX This will look so...  \n",
       "71     @EvaFoxU @SawyerMerritt I love Norway! üá≥üá¥ ‚ô•Ô∏èüá≥üá¥  \n",
       "\n",
       "[72 rows x 5 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"https://raw.githubusercontent.com/kirenz/twitter-tweepy/main/tweets.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>created_at</th>\n",
       "      <th>id</th>\n",
       "      <th>author_id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2021-12-10T07:20:45.000Z</td>\n",
       "      <td>1469205428227784711</td>\n",
       "      <td>44196397</td>\n",
       "      <td>@albi_sidearms maybe i will ‚Ä¶</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2021-12-10T07:19:05.000Z</td>\n",
       "      <td>1469205011687223298</td>\n",
       "      <td>44196397</td>\n",
       "      <td>@jack https://t.co/ueyr6nawap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2021-12-10T06:44:19.000Z</td>\n",
       "      <td>1469196261953884160</td>\n",
       "      <td>44196397</td>\n",
       "      <td>@sawyermerritt ü§£ü§£</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2021-12-10T04:42:00.000Z</td>\n",
       "      <td>1469165476911755264</td>\n",
       "      <td>44196397</td>\n",
       "      <td>@sawyermerritt tesla china has done amazing work</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2021-12-10T04:21:25.000Z</td>\n",
       "      <td>1469160298158383109</td>\n",
       "      <td>44196397</td>\n",
       "      <td>@mrbeast üôè</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>67</td>\n",
       "      <td>2021-12-03T23:35:30.000Z</td>\n",
       "      <td>1466914018615078912</td>\n",
       "      <td>44196397</td>\n",
       "      <td>@joroulette it is an honor to serve nasa and t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>68</td>\n",
       "      <td>2021-12-03T19:28:57.000Z</td>\n",
       "      <td>1466851970443010056</td>\n",
       "      <td>44196397</td>\n",
       "      <td>@nasaspaceflight 39a is hallowed spaceflight g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>69</td>\n",
       "      <td>2021-12-03T19:22:34.000Z</td>\n",
       "      <td>1466850364012044288</td>\n",
       "      <td>44196397</td>\n",
       "      <td>@evafoxu @sawyermerritt huge cranes are cool haha</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>70</td>\n",
       "      <td>2021-12-03T19:20:15.000Z</td>\n",
       "      <td>1466849780253003782</td>\n",
       "      <td>44196397</td>\n",
       "      <td>@ppathole @ercxspace @spacex this will look so...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>71</td>\n",
       "      <td>2021-12-03T19:18:44.000Z</td>\n",
       "      <td>1466849402534907910</td>\n",
       "      <td>44196397</td>\n",
       "      <td>@evafoxu @sawyermerritt i love norway! üá≥üá¥ ‚ô•Ô∏èüá≥üá¥</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>72 rows √ó 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0                created_at                   id  author_id  \\\n",
       "0            0  2021-12-10T07:20:45.000Z  1469205428227784711   44196397   \n",
       "1            1  2021-12-10T07:19:05.000Z  1469205011687223298   44196397   \n",
       "2            2  2021-12-10T06:44:19.000Z  1469196261953884160   44196397   \n",
       "3            3  2021-12-10T04:42:00.000Z  1469165476911755264   44196397   \n",
       "4            4  2021-12-10T04:21:25.000Z  1469160298158383109   44196397   \n",
       "..         ...                       ...                  ...        ...   \n",
       "67          67  2021-12-03T23:35:30.000Z  1466914018615078912   44196397   \n",
       "68          68  2021-12-03T19:28:57.000Z  1466851970443010056   44196397   \n",
       "69          69  2021-12-03T19:22:34.000Z  1466850364012044288   44196397   \n",
       "70          70  2021-12-03T19:20:15.000Z  1466849780253003782   44196397   \n",
       "71          71  2021-12-03T19:18:44.000Z  1466849402534907910   44196397   \n",
       "\n",
       "                                                 text  \n",
       "0                       @albi_sidearms maybe i will ‚Ä¶  \n",
       "1                       @jack https://t.co/ueyr6nawap  \n",
       "2                                   @sawyermerritt ü§£ü§£  \n",
       "3    @sawyermerritt tesla china has done amazing work  \n",
       "4                                          @mrbeast üôè  \n",
       "..                                                ...  \n",
       "67  @joroulette it is an honor to serve nasa and t...  \n",
       "68  @nasaspaceflight 39a is hallowed spaceflight g...  \n",
       "69  @evafoxu @sawyermerritt huge cranes are cool haha  \n",
       "70  @ppathole @ercxspace @spacex this will look so...  \n",
       "71     @evafoxu @sawyermerritt i love norway! üá≥üá¥ ‚ô•Ô∏èüá≥üá¥  \n",
       "\n",
       "[72 rows x 5 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'] = df['text'].astype(str)\n",
    "df['text'] = df['text'].str.lower()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [RegexpTokenizer](https://www.nltk.org/_modules/nltk/tokenize/regexp.html) \n",
    "- [regular expression](https://www.w3schools.com/python/python_regex.asp).\n",
    "- [interactive regular expressions tool](https://regex101.com/)\n",
    "\n",
    "`\\w+` matches Unicode word characters with one or more occurrences; this includes most characters that can be part of a word in any language, as well as numbers and the underscore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                       [albi_sidearms, maybe, i, will]\n",
       "1                      [jack, https, t, co, ueyr6nawap]\n",
       "2                                       [sawyermerritt]\n",
       "3     [sawyermerritt, tesla, china, has, done, amazi...\n",
       "4                                             [mrbeast]\n",
       "                            ...                        \n",
       "67    [joroulette, it, is, an, honor, to, serve, nas...\n",
       "68    [nasaspaceflight, 39a, is, hallowed, spaceflig...\n",
       "69    [evafoxu, sawyermerritt, huge, cranes, are, co...\n",
       "70    [ppathole, ercxspace, spacex, this, will, look...\n",
       "71            [evafoxu, sawyermerritt, i, love, norway]\n",
       "Name: text_token, Length: 72, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "regexp = RegexpTokenizer('\\w+')\n",
    "\n",
    "df['text_token']=df['text'].apply(regexp.tokenize)\n",
    "df['text_token']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopwords\n",
    "\n",
    "If you use this module the first time, you need to install stopwords:\n",
    "\n",
    "```python\n",
    "import nltk\n",
    "\n",
    "nltk.download(‚Äòstopwords‚Äô)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# make a list of german stopwords\n",
    "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "\n",
    "# extend the list with your own custom stopwords\n",
    "my_stopwords = ['https']\n",
    "stopwords.extend(my_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove stopwords\n",
    "df['text_token'] = df['text_token'].apply(lambda x: [item for item in x if item not in stopwords])\n",
    "df['text_token'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove infrequent words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remove all words that have a length <=2. In general, small words (length <=2 ) aren‚Äôt useful for sentiment analysis because they have no meaning. These most probably are noise in our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_token'] = df['text_token'].apply(lambda x: ' '.join([w for w in x if len(w)>2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_token']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "wordnet_lem = WordNetLemmatizer()\n",
    "\n",
    "df['text_token'] = df['text_token'].apply(wordnet_lem.lemmatize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_token']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word cloud\n",
    "\n",
    "[Word cloud example gallery](https://amueller.github.io/word_cloud/auto_examples/index.html#example-gallery)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = ' '.join([word for word in df['text_token']])\n",
    "all_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "wordcloud = WordCloud(width=600, \n",
    "                     height=400, \n",
    "                     random_state=2, \n",
    "                     max_font_size=100).generate(all_words)\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different style:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "x, y = np.ogrid[:300, :300]\n",
    "mask = (x - 150) ** 2 + (y - 150) ** 2 > 130 ** 2\n",
    "mask = 255 * mask.astype(int)\n",
    "\n",
    "wc = WordCloud(background_color=\"white\", repeat=True, mask=mask)\n",
    "wc.generate(all_words)\n",
    "\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(wc, interpolation=\"bilinear\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist\n",
    "\n",
    "words = nltk.tokenize.word_tokenize(all_words)\n",
    "fd = FreqDist(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most common words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fd.most_common(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fd.tabulate(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain top 10 words\n",
    "top_10 = fd.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set_theme(style=\"ticks\")\n",
    "\n",
    "# Make pandas series for easier plotting\n",
    "fdist = pd.Series(dict(top_10))\n",
    "\n",
    "## Seaborn plotting using Pandas attributes + xtick rotation for ease of viewing\n",
    "sns.barplot(y=fdist.index, x=fdist.values);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fd[\"nasa\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment import SentimentIntensityAnalyzer\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "463226f144cc21b006ce6927bfc93dd00694e52c8bc6857abb6e555b983749e9"
  },
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
